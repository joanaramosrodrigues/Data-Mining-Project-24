{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Índice \n",
    "1. [Introduction](#introduction)\n",
    "2. [Import Materials](#importmaterials)    \n",
    "    2.1. [Import Libraries](#importlibraries)     \n",
    "    2.2. [Import the dataset](#importdataset)\n",
    "3. [Clusterings](#clustering)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"introduction\">\n",
    "    \n",
    "# 1. Introduction\n",
    "    \n",
    "</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook analyzes customer data from ABCDEats Inc., a fictional food delivery service, over three months. The goal is to create clusters of customers and make different marketing startegies for each of them....    \n",
    "**Project by** Dinis Pinto (20240612), Joana Rodrigues (20240603), João Marques (20240656), and Mara Simões (20240326) - **Group 27**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"importmaterials\">\n",
    "    \n",
    "# 2. Import Materials\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"importlibraries\">\n",
    "    \n",
    "## 2.1. Import Libraries\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, we'll install some helpful libraries to make analysis easier and add features like better plotting, data handling, and modeling tools. For example, `matplotlib` and `seaborn` allow us to create clear, customizable plots, while `pandas` and `numpy` make data processing smoother."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import pickle\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.model_selection import train_test_split\n",
    "#pip install umap-learn\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from minisom import MiniSom\n",
    "import umap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"importdataset\">\n",
    "    \n",
    "## 2.2. Import Dataset\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data in dataset\n",
    "The different columns in the dataset give out the following information:   \n",
    "- `customer_id` - Unique identifier for each customer.\n",
    "- `customer_region` - Geographic region where the customer is located.\n",
    "- `customer_age` - Age of the customer.\n",
    "- `vendor_count` - Number of unique vendors the customer has ordered from.\n",
    "- `product_count` - Total number of products the customer has ordered.\n",
    "- `is_chain` - Indicates whether the customer’s order was from a chain restaurant.\n",
    "- `first_order` - Number of days from the start of the dataset when the customer first placed an order.\n",
    "- `last_order` - Number of days from the start of the dataset when the customer most recently placed an order.\n",
    "- `last_promo` - The category of the promotion or discount most recently used by the customer.\n",
    "- `payment_method` - Method most recently used by the customer to pay for their orders.\n",
    "- `CUI_American`, `CUI_Asian`, `CUI_Chinese`, `CUI_Italian`... - The amount in monetary units spent by the customer from the indicated type of cuisine. \n",
    "- `DOW_0` to `DOW_6` - Number of orders placed on each day of the week (0 = Sunday, 6 = Saturday).\n",
    "- `HR_0` to `HR_23` - Number of orders placed during each hour of the day (0 = midnight, 23 = 11 PM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_behaviour = pickle.load(open(\"df_behaviour.pkl\", 'rb'))\n",
    "# df_time = pickle.load(open(\"df_time.pkl\", 'rb'))\n",
    "df_preferences = pickle.load(open(\"df_preferences.pkl\", 'rb'))\n",
    "# df_preferences_grouped = pickle.load(open(\"df_preferences_grouped.pkl\", 'rb'))\n",
    "\n",
    "df = pickle.load(open(\"df_final.pkl\", 'rb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the analysis we remove:\n",
    "- all the DOW_ as the information is not varying a lot and is already present by being divided in days of the week and weekend.\n",
    "- all the HR_ because the information doesn't vary a lot between them and would lead to a lot of information that can also be seen and analysed through the groups made of hours\n",
    "- customer_age because as a numeric varible all clusters would have its mean (27.5) as its distribution is normal, so we put them into groups so we can better try to understand which cluster contains each age group, so we transformed age into a categorical column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[:, ~df.columns.str.startswith(('DOW_', 'HR_', 'customer_age', 'asian_cuisines', 'american_cuisines', 'cafe_desserts'))] \n",
    "not_used_columns = [col for col in df.columns if col not in df_preferences.columns and col not in df_behaviour.columns]\n",
    "categorical_columns = ['customer_region', 'last_promo', 'payment_method', 'age_range', 'customer_city']\n",
    "numeric_columns = [col for col in df if col not in categorical_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Optimal clusters for each segmentation\n",
    "From notebook part 2 and 3 we chose the best clustering approach for each of the segmentation after exploring several different ones.\n",
    "## 3.1. df_behaviour - 2 clusters with K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmclust_behaviour = KMeans(n_clusters=2, init='k-means++', n_init=15, random_state=1)\n",
    "km_labels = kmclust_behaviour.fit_predict(df_behaviour)\n",
    "\n",
    "df_concat = pd.concat((df_behaviour, pd.Series(km_labels, name='labels', index=df_behaviour.index)), axis=1)\n",
    "df_concat.groupby('labels').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. df_preferences - ???\n",
    "To avoid having to repeat code already done we imported the pickle with the clustering already done as SOM is more computationally expensive than K-Means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmclust_pref = KMeans(n_clusters=6, init='k-means++', n_init=15, random_state=1)\n",
    "km_labels = kmclust_behaviour.fit_predict(df_preferences)\n",
    "\n",
    "df_concat = pd.concat((df_behaviour, pd.Series(km_labels, name='labels', index=df_preferences.index)), axis=1)\n",
    "df_concat.groupby('labels').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Merge\n",
    "In this section we will merge the clusters of the two segmentations.   \n",
    "The following table shows the 4*3 clusters formed from this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the right clustering (algorithm and number of clusters) for each perspective\n",
    "\n",
    "behavior_labels = kmclust_behaviour.fit_predict(df_behaviour)\n",
    "pref_labels = kmclust_pref.fit_predict(df_preferences)\n",
    "\n",
    "df['behavior_labels'] = behavior_labels\n",
    "df['preference_labels'] = pref_labels\n",
    "\n",
    "crosstab_df = pd.crosstab(df['behavior_labels'],\n",
    "            df['preference_labels'])\n",
    "crosstab_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1. Reformulation of clusters\n",
    "From the previous code we can see that some of the clusters formed have some clusters with few points, this way our goal is to join this points as it doesnt make sense to be doing specific marketing strategies for a very neesh group of people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Determining the threshold from which the clusters are joined\n",
    "threshold = len(df_behaviour) * 0.045  \n",
    "\n",
    "# Identify clusters smaller than the threshold\n",
    "to_merge = []\n",
    "for behavior_label, row in crosstab_df.iterrows():\n",
    "    for preference_label, count in row.items():\n",
    "        if count is not None and count < threshold:  # Check for None values\n",
    "            to_merge.append((behavior_label, preference_label))\n",
    "\n",
    "# Output results\n",
    "print(\"Threshold:\", threshold)\n",
    "print(\"Clusters to merge:\", to_merge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_centroids = df.groupby(['behavior_labels', 'preference_labels'])\\\n",
    "    [numeric_columns].mean()\n",
    "df_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Computing the euclidean distance matrix between the centroids to join the closests\n",
    "centroid_dists = euclidean = pairwise_distances(df_centroids)\n",
    "\n",
    "df_dists = pd.DataFrame(\n",
    "    centroid_dists, \n",
    "    columns=df_centroids.index, \n",
    "    index=df_centroids.index\n",
    ")\n",
    "\n",
    "df_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Merging each low frequency clustering (source) to the closest cluster (target)\n",
    "source_target = {}\n",
    "\n",
    "for clus in to_merge:\n",
    "    # If the current cluster has not already been used as a target\n",
    "    if clus not in source_target.values():\n",
    "        # Find the closest cluster that is not in `to_merge`\n",
    "        possible_targets = df_dists.loc[clus].sort_values()\n",
    "        for potential_target in possible_targets.index[1:]:  # Skip self (index[0])\n",
    "            if potential_target not in to_merge:\n",
    "                source_target[clus] = potential_target\n",
    "                break\n",
    "\n",
    "print(source_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = df.copy()\n",
    "\n",
    "# Changing the behavior_labels and product_labels based on source_target\n",
    "for source, target in source_target.items():\n",
    "    mask = (df_['behavior_labels']==source[0]) & (df_['preference_labels']==source[1])\n",
    "    df_.loc[mask, 'behavior_labels'] = target[0]\n",
    "    df_.loc[mask, 'preference_labels'] = target[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df_['behavior_labels'],\n",
    "            df_['preference_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Combine behavior_labels and preference_labels into a tuple\n",
    "df_['behavior_preference_pair'] = list(zip(df_['behavior_labels'], df_['preference_labels']))\n",
    "\n",
    "# Step 2: Get the unique combinations of (behavior_labels, preference_labels)\n",
    "unique_combinations = df_['behavior_preference_pair'].unique()\n",
    "\n",
    "# Step 3: Manually assign a unique value to each unique combination (initial mapping)\n",
    "initial_mapping = {comb: idx for idx, comb in enumerate(unique_combinations)}\n",
    "\n",
    "\n",
    "# Step 5: Update the initial mapping based on the merging\n",
    "# Create a new mapping where the merged clusters will take the same label as their target\n",
    "merged_mapping = initial_mapping.copy()\n",
    "\n",
    "# Helper function to find the final target label (propagating merging)\n",
    "def find_target_label(cluster, source_target, merged_mapping):\n",
    "    # Traverse the chain until the final target cluster is found\n",
    "    while cluster in source_target:\n",
    "        cluster = source_target[cluster]\n",
    "    # Return the final label of the target\n",
    "    return merged_mapping.get(cluster, cluster)\n",
    "\n",
    "# Loop over each source-target pair in the source_target dictionary\n",
    "for source, target in source_target.items():\n",
    "    # Find the final target label and propagate it for the source cluster\n",
    "    merged_mapping[source] = find_target_label(target, source_target, merged_mapping)\n",
    "\n",
    "# Step 6: Print the full merged mapping\n",
    "print( merged_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Map the unique combination to its corresponding value\n",
    "df_['merged_labels'] = df_.apply(\n",
    "    lambda row: merged_mapping.get(\n",
    "        (row['behavior_labels'], row['preference_labels']), -1\n",
    "    ), axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_.groupby('merged_labels').mean(numeric_only=True)[numeric_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge cluster contigency table\n",
    "# Getting size of each final cluster\n",
    "df_counts = df_.groupby('merged_labels')\\\n",
    "    .size()\\\n",
    "    .to_frame()\n",
    "\n",
    "# df_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the product and behavior labels\n",
    "df_counts = df_counts\\\n",
    "    .rename({v:k for k, v in merged_mapping.items()})\\\n",
    "    .reset_index()\n",
    "\n",
    "df_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts['behavior_labels'] = df_counts['merged_labels'].apply(lambda x: x[0])\n",
    "df_counts['preference_labels'] = df_counts['merged_labels'].apply(lambda x: x[1])\n",
    "\n",
    "df_counts\n",
    "df_counts.pivot(values=0, index='behavior_labels', columns='preference_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts.pivot(values=0, index='behavior_labels', columns='preference_labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Cluster analysis and customer profiling\n",
    "## 4.1.Numerical features\n",
    "This section will be useful to describe the clusters in the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_profiles(df, label_columns, figsize, cmap=\"tab10\", compare_titles=None):\n",
    "    \"\"\"\n",
    "    Pass df with labels columns of one or multiple clustering labels. \n",
    "    Then specify these label columns to perform the cluster profile according to them.\n",
    "    \"\"\"\n",
    "    if compare_titles is None:\n",
    "        compare_titles = [\"\"] * len(label_columns)\n",
    "        \n",
    "    fig, axes = plt.subplots(nrows=len(label_columns), \n",
    "                             ncols=2, \n",
    "                             figsize=figsize, \n",
    "                             constrained_layout=True,\n",
    "                             squeeze=False)\n",
    "    \n",
    "    for ax, label, titl in zip(axes, label_columns, compare_titles):\n",
    "        # Filtering df to exclude label columns\n",
    "        drop_cols = [i for i in label_columns if i != label]\n",
    "        dfax = df.drop(drop_cols, axis=1)\n",
    "        \n",
    "        # Get only numeric columns for clustering\n",
    "        numeric_dfax = dfax.select_dtypes(include=['number'])\n",
    "\n",
    "        # Getting the cluster centroids and counts\n",
    "        centroids = numeric_dfax.groupby(by=label, as_index=False).mean()  # Compute mean for numeric columns only\n",
    "        counts = dfax.groupby(by=label).size().reset_index(name=\"counts\")\n",
    "        \n",
    "        # Plotting\n",
    "        pd.plotting.parallel_coordinates(centroids, \n",
    "                                            label, \n",
    "                                            color=sns.color_palette(cmap),\n",
    "                                            ax=ax[0])\n",
    "\n",
    "        sns.barplot(x=label, \n",
    "                    hue=label,\n",
    "                    y=\"counts\", \n",
    "                    data=counts, \n",
    "                    ax=ax[1], \n",
    "                    palette=sns.color_palette(cmap),\n",
    "                    legend=False)\n",
    "\n",
    "        # Setting Layout\n",
    "        handles, _ = ax[0].get_legend_handles_labels()\n",
    "        cluster_labels = [\"Cluster {}\".format(i) for i in range(len(handles))]\n",
    "        ax[0].annotate(text=titl, xy=(0.95, 1.1), xycoords='axes fraction', fontsize=13, fontweight='heavy') \n",
    "        ax[0].axhline(color=\"black\", linestyle=\"--\")\n",
    "        ax[0].set_title(f\"Cluster Means - {len(handles)} Clusters\", fontsize=13)\n",
    "        ax[0].set_xticklabels(ax[0].get_xticklabels(), rotation=40, ha='right')\n",
    "        \n",
    "        ax[0].legend(handles, cluster_labels,\n",
    "                     loc='center left', bbox_to_anchor=(1, 0.5), title=label)\n",
    "\n",
    "        ax[1].set_xticks([i for i in range(len(handles))])\n",
    "        ax[1].set_xticklabels(cluster_labels)\n",
    "        ax[1].set_xlabel(\"\")\n",
    "        ax[1].set_ylabel(\"Absolute Frequency\")\n",
    "        ax[1].set_title(f\"Cluster Sizes - {len(handles)} Clusters\", fontsize=13)\n",
    "        \n",
    "    plt.suptitle(\"Cluster Simple Profiling\", fontsize=23)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_.copy()\n",
    "\n",
    "# Apply the cluster profiling function\n",
    "cluster_profiles(\n",
    "    df=df[numeric_columns + [ 'behavior_labels','preference_labels', 'merged_labels']], \n",
    "    label_columns=['behavior_labels','preference_labels', 'merged_labels'], \n",
    "    figsize=(28, 13), \n",
    "    compare_titles=[ \"Behavior clustering\", \"Preference clustering\",\"Merged clusters\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_used_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by the cluster labels and summing for the categorical features\n",
    "df_cat = df[categorical_columns + ['merged_labels']].groupby(['merged_labels']).apply(lambda x: x.apply(lambda col: col.value_counts().to_dict(), axis=0))\n",
    "\n",
    "# For visualizing, let's assume you want to create count plots for each categorical feature by cluster\n",
    "fig, axes = plt.subplots(len(categorical_columns), df['merged_labels'].nunique() + 1, figsize=(16, 4 * len(categorical_columns)), tight_layout=True)\n",
    "\n",
    "for i, feature in enumerate(categorical_columns):\n",
    "    for j in range(len(axes[i])):\n",
    "        ax = axes[i][j]\n",
    "        if j == 0:\n",
    "            sns.countplot(data=df, x=feature, order=df[feature].value_counts().index, ax=ax)\n",
    "            ax.set_title(f\"All Data - {feature}\")\n",
    "        else:\n",
    "            sns.countplot(data=df[df['merged_labels'] == j - 1], x=feature, order=df[feature].value_counts().index, ax=ax)\n",
    "            ax.set_title(f\"Cluster {j - 1}\")\n",
    "        \n",
    "        ax.tick_params(axis=\"x\", labelrotation=90)\n",
    "        ax.set_xlabel(\"\")\n",
    "        ax.set_ylabel(\"\")\n",
    "\n",
    "plt.suptitle(\"Categorical Feature Counts by Clusters\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each categorical feature to create both count and percentage stacked bar charts\n",
    "for feature in not_used_columns:\n",
    "    # Grouping by 'merged_labels' and the feature, then counting occurrences\n",
    "    df_cl = df.groupby(['merged_labels', feature])[feature].size().unstack(fill_value=0)\n",
    "    \n",
    "    # Create the counts plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6), tight_layout=True)\n",
    "    \n",
    "    # Count plot on the left\n",
    "    df_cl.plot.bar(stacked=True, ax=axes[0], title=f'{feature} Count Distribution by Clusters')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].set_xlabel('Cluster Labels')\n",
    "    axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=0)  # Optional for better x-axis label visibility\n",
    "    \n",
    "    # Calculate the percentage distribution\n",
    "    df_cl_pct = df_cl.copy()\n",
    "    \n",
    "    # Handle missing values by making sure all feature categories are considered\n",
    "    # Note: We need to make sure each category in 'feature' has the right percentage calculation.\n",
    "    total_counts = df['merged_labels'].value_counts().sort_index()\n",
    "    \n",
    "    for category in df[feature].unique():\n",
    "        if category in df_cl_pct.columns:\n",
    "            df_cl_pct[category] = 100 * df_cl_pct[category] / total_counts.values\n",
    "    \n",
    "    # Percentage plot on the right\n",
    "    df_cl_pct.plot.bar(stacked=True, ax=axes[1], title=f'{feature} Percentage Distribution by Clusters')\n",
    "    axes[1].set_ylabel('Percentage')\n",
    "    axes[1].set_xlabel('Cluster Labels')\n",
    "    axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=0)  # Optional for better x-axis label visibility\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.Assessment of clutering made\n",
    "## 5.1. T-sne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_dim = TSNE(random_state=42).fit_transform(df[numeric_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(two_dim).plot.scatter(x=0, y=1, c=df['merged_labels'], colormap='tab10', figsize=(15,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply UMAP\n",
    "umap_model = umap.UMAP(random_state=42)\n",
    "two_dim_umap = umap_model.fit_transform(df[numeric_columns])\n",
    "\n",
    "# Plot the UMAP results\n",
    "pd.DataFrame(two_dim_umap).plot.scatter(x=0, y=1, c=df['merged_labels'], colormap='tab10', figsize=(15, 10))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA\n",
    "pca_model = PCA(n_components=2, random_state=42)\n",
    "two_dim_pca = pca_model.fit_transform(df[numeric_columns])\n",
    "\n",
    "# Plot the PCA results\n",
    "pd.DataFrame(two_dim_pca).plot.scatter(x=0, y=1, c=df['merged_labels'], colormap='tab10', figsize=(15, 10))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ss_variables(df):\n",
    "    \"\"\"Get the SS for each variable\n",
    "    \"\"\"\n",
    "    ss_vars = df.var() * (df.count() - 1)\n",
    "    return ss_vars\n",
    "    \n",
    "\n",
    "def r2_variables(df, labels):\n",
    "    \"\"\"Get the R² for each variable\n",
    "    \"\"\"\n",
    "    sst_vars = get_ss_variables(df)\n",
    "    ssw_vars = np.sum(df.groupby(labels).apply(get_ss_variables))\n",
    "    return 1 - ssw_vars/sst_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are essentially decomposing the R² into the R² for each variable\n",
    "r2_variables(df[numeric_columns + ['merged_labels']], 'merged_labels').drop('merged_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the data\n",
    "X = df[numeric_columns]\n",
    "y = df.merged_labels\n",
    "\n",
    "# Splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Fitting the decision tree\n",
    "dt = DecisionTreeClassifier(random_state=42, max_depth=3)\n",
    "dt.fit(X_train, y_train)\n",
    "print(\"It is estimated that in average, we are able to predict {0:.2f}% of the customers correctly\".format(dt.score(X_test, y_test)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assessing feature importance\n",
    "pd.Series(dt.feature_importances_, index=X_train.columns).sort_values(ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
